{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\t\t\t\t\t\tsample_submission.csv  trn\r\n",
      "models\t\t\t\t\t\ttest.csv\t       val\r\n",
      "PredictHappinessModel_17_11_28_03_ensemble.csv\ttmp\r\n",
      "PredictHappinessModel_Final_ensemble.csv\ttrain.csv\r\n"
     ]
    }
   ],
   "source": [
    "PATH='data/predictHappiness/'\n",
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = pd.read_csv(PATH+\"train.csv\")\n",
    "tst = pd.read_csv(PATH+\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the structure of the folders I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in trn.Is_Response.unique():\n",
    "#    trn_dir_path = f'{PATH}trn/{i.replace(\" \",\"_\")}'\n",
    "#    val_dir_path = f'{PATH}val/{i.replace(\" \",\"_\")}'\n",
    "#    !rm -r {trn_dir_path}\n",
    "#    !rm -r {val_dir_path}\n",
    "#    os.makedirs(trn_dir_path, exist_ok=True)\n",
    "#    os.makedirs(val_dir_path, exist_ok=True)\n",
    "#!rm -r {PATH}all/trn\n",
    "#!rm -r {PATH}all/val\n",
    "#os.makedirs(f'{PATH}all/trn', exist_ok=True)\n",
    "#os.makedirs(f'{PATH}all/val', exist_ok=True)\n",
    "#os.makedirs(f'{PATH}all/tst', exist_ok=True)\n",
    "#os.makedirs(f'{PATH}models', exist_ok=True)\n",
    "#os.makedirs(f'{PATH}/models/Stage1Model', exist_ok=True)\n",
    "#os.makedirs(f'{PATH}/models/Stage2Model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the csv in individual files in folders that split them into happy and not_happy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range(trn.values[:,1].shape[0]):\n",
    "#    dset = 'trn' if random.random()>0.05 else 'val'\n",
    "#    f = open(f\"{PATH}all/{dset}/\"+trn.values[i,0]+\".txt\", 'w')\n",
    "#    f.write(trn.values[i,1])\n",
    "#    f.close()\n",
    "#    f = open(f\"{PATH}{dset}/\"+trn.Is_Response[i].replace(\" \",\"_\")+\"/\"+trn.values[i,0]+\".txt\", 'w')\n",
    "#    f.write(trn.values[i,1])\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range(tst.values[:,1].shape[0]):\n",
    "#    f = open(PATH+\"all/tst/\"+tst.values[i,0]+\".txt\", 'w')\n",
    "#    f.write(tst.values[i,1])\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull all the words in using the spacy tokenizer and set lower to True (One step I wish I had more time to play with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=spacy_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64; bptt=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train=\"all/trn/\", validation=\"all/val/\", test=\"all/tst/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_freq is set to 10 so anything that occurs less than 10 times won't show up.  I would like to increase this to 25 or 50 but 10 seemed to do the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = LanguageModelData(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200  # size of each embedding vector\n",
    "nh = 500     # number of hidden activations per layer\n",
    "nl = 3       # number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first model is just to be able to learn what words make sense together and to be able to build a coherent sentence.  This ensures that the embedding weights are reasonable.  This was a place that I found made a big difference in my final score.  When I trained this a few times I was getting ok scores, but when I trained it more and added the cycle_mult which brings in cyclicle annealing I was able to get some of my best scores.  One way I know I could get a higher score is to add another model to my final ensemble with one more round of training.  This did very well by itself, but I never was able to add it to the final ensemble because the weights were lost and I didn't have time to retrain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiplier=1 #play around with this\n",
    "#dropouti=0.05*multiplier\n",
    "#dropout=0.05*multiplier\n",
    "#wdrop=0.1*multiplier\n",
    "#dropoute=0.02*multiplier\n",
    "#dropouth=0.05*multiplier\n",
    "#learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "#               dropouti=dropouti, dropout=dropout, wdrop=wdrop, dropoute=dropoute, dropouth=dropouth)\n",
    "#learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "#learner.clip=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496cda9d372d4f4383e5aa08fed4f92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       4.65618  4.6425 ]                                  \n",
      "[ 1.       3.83107  3.76976]                                  \n",
      "[ 2.       3.68609  3.64447]                                  \n",
      "[ 3.       3.59806  3.52884]                                  \n",
      "[ 4.       3.47371  3.44122]                                  \n",
      "[ 5.       3.40985  3.38482]                                  \n",
      "[ 6.       3.36285  3.37347]                                  \n",
      "[ 7.       3.45098  3.41205]                                  \n",
      "[ 8.       3.40512  3.37419]                                  \n",
      "[ 9.       3.36023  3.34766]                                  \n",
      "[ 10.        3.33518   3.31689]                               \n",
      "[ 11.        3.2799    3.28944]                               \n",
      "[ 12.        3.24986   3.28134]                               \n",
      "[ 13.        3.22152   3.27062]                               \n",
      "[ 14.        3.21745   3.26814]                               \n",
      "[ 15.        3.35704   3.33033]                               \n",
      "[ 16.        3.33451   3.32273]                               \n",
      "[ 17.        3.34014   3.31231]                               \n",
      "[ 18.        3.29143   3.29725]                               \n",
      "[ 19.        3.29729   3.29392]                               \n",
      "[ 20.        3.25623   3.27872]                               \n",
      "[ 21.        3.234     3.26703]                               \n",
      "[ 22.        3.245     3.25295]                               \n",
      "[ 23.        3.21692   3.24516]                               \n",
      "[ 24.        3.17836   3.23872]                               \n",
      "[ 25.        3.157     3.23096]                               \n",
      "[ 26.        3.16477   3.2255 ]                               \n",
      "[ 27.        3.13787   3.22135]                               \n",
      "[ 28.        3.16028   3.21338]                               \n",
      "[ 29.        3.11785   3.21908]                               \n",
      "[ 30.        3.12312   3.21671]                               \n",
      "[ 31.        3.28872   3.28532]                               \n",
      "[ 32.        3.27183   3.28052]                               \n",
      "[ 33.        3.26909   3.27369]                               \n",
      "[ 34.        3.25359   3.2732 ]                               \n",
      "[ 35.        3.26164   3.26145]                               \n",
      "[ 36.        3.23876   3.26088]                               \n",
      "[ 37.        3.22209   3.25783]                               \n",
      "[ 38.        3.21824   3.25309]                               \n",
      "[ 39.        3.22075   3.25309]                               \n",
      "[ 40.        3.20034   3.24627]                               \n",
      "[ 41.        3.20158   3.24254]                               \n",
      "[ 42.        3.18272   3.2359 ]                               \n",
      "[ 43.        3.18499   3.23173]                               \n",
      "[ 44.        3.17006   3.22531]                               \n",
      "[ 45.        3.15394   3.22375]                               \n",
      "[ 46.        3.16083   3.22069]                               \n",
      "[ 47.        3.14062   3.21697]                               \n",
      "[ 48.        3.11697   3.21203]                               \n",
      "[ 49.        3.10925   3.2081 ]                               \n",
      "[ 50.        3.09125   3.2079 ]                               \n",
      "[ 51.        3.1387    3.19868]                               \n",
      "[ 52.        3.07908   3.20439]                               \n",
      "[ 53.        3.07444   3.19652]                               \n",
      "[ 54.        3.08294   3.19551]                               \n",
      "[ 55.        3.0772    3.19111]                               \n",
      "[ 56.        3.04147   3.19222]                               \n",
      "[ 57.        3.03317   3.19613]                               \n",
      "[ 58.        3.03759   3.18992]                               \n",
      "[ 59.        3.03588   3.19161]                               \n",
      "[ 60.        3.04172   3.18897]                               \n",
      "[ 61.        3.02671   3.19081]                               \n",
      "[ 62.        3.02706   3.18961]                               \n",
      "[ 63.        3.23641   3.24665]                               \n",
      "[ 64.        3.21678   3.24672]                               \n",
      "[ 65.        3.21531   3.2464 ]                               \n",
      "[ 66.        3.20832   3.25188]                               \n",
      "[ 67.        3.20164   3.25037]                               \n",
      "[ 68.        3.20036   3.24424]                               \n",
      "[ 69.        3.20513   3.24295]                               \n",
      "[ 70.        3.20491   3.23863]                               \n",
      "[ 71.        3.20081   3.24326]                               \n",
      "[ 72.        3.1873    3.23445]                               \n",
      "[ 73.        3.18798   3.23434]                               \n",
      "[ 74.        3.20422   3.23484]                               \n",
      "[ 75.        3.17584   3.22932]                               \n",
      "[ 76.        3.17641   3.23319]                               \n",
      "[ 77.        3.17016   3.22977]                               \n",
      "[ 78.        3.15715   3.2256 ]                               \n",
      "[ 79.        3.1665    3.22565]                               \n",
      "[ 80.        3.14969   3.22603]                               \n",
      "[ 81.        3.18738   3.21984]                               \n",
      "[ 82.        3.14544   3.21947]                               \n",
      "[ 83.        3.14042   3.21644]                               \n",
      "[ 84.        3.14865   3.21563]                               \n",
      "[ 85.        3.13204   3.21514]                               \n",
      "[ 86.        3.13052   3.21386]                               \n",
      "[ 87.        3.13775   3.21085]                               \n",
      "[ 88.        3.11778   3.21568]                               \n",
      "[ 89.        3.14095   3.20716]                               \n",
      "[ 90.        3.11289   3.20602]                               \n",
      "[ 91.        3.09942   3.20308]                               \n",
      "[ 92.        3.08856   3.20613]                               \n",
      "[ 93.        3.12061   3.20517]                               \n",
      "[ 94.        3.08941   3.19782]                               \n",
      "[ 95.        3.11403   3.19929]                               \n",
      "[ 96.        3.07311   3.19607]                               \n",
      "[ 97.        3.07342   3.19371]                               \n",
      "[ 98.        3.05535   3.1977 ]                               \n",
      "[ 99.        3.05885   3.19249]                               \n",
      "[ 100.         3.05658    3.1935 ]                            \n",
      "[ 101.         3.05624    3.18764]                            \n",
      "[ 102.         3.03787    3.19158]                            \n",
      "[ 103.         3.09354    3.18593]                            \n",
      "[ 104.         3.03904    3.18553]                            \n",
      "[ 105.         3.03607    3.18323]                            \n",
      "[ 106.         3.01344    3.18692]                            \n",
      "[ 107.         3.01516    3.18578]                            \n",
      "[ 108.         3.01227    3.18472]                            \n",
      "[ 109.         3.00547    3.18517]                            \n",
      "[ 110.         2.99474    3.18416]                            \n",
      "[ 111.         2.98583    3.18636]                            \n",
      "[ 112.         2.98938    3.18537]                            \n",
      "[ 113.         3.00153    3.18246]                            \n",
      "[ 114.         2.97006    3.18276]                            \n",
      "[ 115.         2.97567    3.18402]                            \n",
      "[ 116.         2.97876    3.18426]                            \n",
      "[ 117.         2.97854    3.18328]                            \n",
      "[ 118.         2.96917    3.18218]                            \n",
      "[ 119.         2.96901    3.18436]                            \n",
      "[ 120.         2.95796    3.18277]                            \n",
      "[ 121.         2.9615     3.18249]                            \n",
      "[ 122.         2.96448    3.18135]                            \n",
      "[ 123.         2.95665    3.18226]                            \n",
      "[ 124.         3.03734    3.18314]                            \n",
      "[ 125.         2.97108    3.18284]                            \n",
      "[ 126.         2.95484    3.18354]                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learner.fit(3e-3, 7, wds=1e-6, cycle_len=1, cycle_mult=2, cycle_save_name=\"PtH_Stage1_cyclemult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.load_cycle(\"PtH_Stage1_cyclemult\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this as weights for md3 which is model 1 for the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.save_encoder('PtH_Stage1_cyclemult_newtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I am going to train it another long while and save the weights again for another.  I believe if I kept doing this I would keep seeing marginal improvements but decided to just do two for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ebcdc4a8944aa0a85efdb6d8760081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       3.0863   3.18537]                                  \n",
      "[ 1.       3.15896  3.203  ]                                  \n",
      "[ 2.       3.0592   3.18375]                                  \n",
      "[ 3.       3.20134  3.22471]                                  \n",
      "[ 4.       3.12271  3.19833]                                  \n",
      "[ 5.       3.05572  3.1796 ]                                  \n",
      "[ 6.       3.01982  3.17836]                                  \n",
      "[ 7.       3.19351  3.22282]                                  \n",
      "[ 8.       3.16917  3.22355]                                  \n",
      "[ 9.       3.16295  3.2084 ]                                  \n",
      "[ 10.        3.11239   3.19955]                               \n",
      "[ 11.        3.06802   3.18485]                               \n",
      "[ 12.        3.02773   3.18085]                               \n",
      "[ 13.        3.01139   3.17454]                               \n",
      "[ 14.        3.00529   3.17281]                               \n",
      "[ 15.        3.19182   3.22998]                               \n",
      "[ 16.        3.18468   3.23286]                               \n",
      "[ 17.        3.17671   3.22474]                               \n",
      "[ 18.        3.16994   3.21274]                               \n",
      "[ 19.        3.14088   3.21085]                               \n",
      "[ 20.        3.11632   3.2095 ]                               \n",
      "[ 21.        3.11118   3.19778]                               \n",
      "[ 22.        3.0921    3.19786]                               \n",
      "[ 23.        3.06485   3.18958]                               \n",
      "[ 24.        3.0444    3.18372]                               \n",
      "[ 25.        3.03231   3.17783]                               \n",
      "[ 26.        3.03065   3.17283]                               \n",
      "[ 27.        3.00269   3.17446]                               \n",
      "[ 28.        3.00103   3.17416]                               \n",
      "[ 29.        2.98787   3.17519]                               \n",
      "[ 30.        2.97957   3.17758]                               \n",
      "[ 31.        3.18474   3.22161]                               \n",
      "[ 32.        3.20107   3.22194]                               \n",
      "[ 33.        3.16641   3.22684]                               \n",
      "[ 34.        3.1666    3.21906]                               \n",
      "[ 35.        3.16154   3.22109]                               \n",
      "[ 36.        3.14809   3.219  ]                               \n",
      "[ 37.        3.17793   3.21483]                               \n",
      "[ 38.        3.14135   3.21016]                               \n",
      "[ 39.        3.15046   3.21113]                               \n",
      "[ 40.        3.12591   3.20797]                               \n",
      "[ 41.        3.11741   3.20744]                               \n",
      "[ 42.        3.14508   3.19738]                               \n",
      "[ 43.        3.09602   3.19695]                               \n",
      "[ 44.        3.12501   3.18897]                               \n",
      "[ 45.        3.08361   3.19297]                               \n",
      "[ 46.        3.07401   3.18495]                               \n",
      "[ 47.        3.05427   3.18675]                               \n",
      "[ 48.        3.05319   3.18455]                               \n",
      "[ 49.        3.07092   3.17667]                               \n",
      "[ 50.        3.02719   3.17801]                               \n",
      "[ 51.        3.01334   3.18208]                               \n",
      "[ 52.        3.01108   3.17115]                               \n",
      "[ 53.        3.02813   3.17416]                               \n",
      "[ 54.        3.00384   3.17116]                               \n",
      "[ 55.        2.96838   3.17516]                               \n",
      "[ 56.        2.98469   3.16915]                               \n",
      "[ 57.        2.96872   3.17436]                               \n",
      "[ 58.        2.99158   3.16918]                               \n",
      "[ 59.        2.95549   3.17333]                               \n",
      "[ 60.        2.9506    3.17162]                               \n",
      "[ 61.        2.95003   3.17106]                               \n",
      "[ 62.        2.96316   3.17006]                               \n",
      "[ 63.        3.16781   3.22168]                               \n",
      "[ 64.        3.16074   3.21453]                               \n",
      "[ 65.        3.16824   3.21577]                               \n",
      "[ 66.        3.15398   3.21857]                               \n",
      "[ 67.        3.18417   3.21198]                               \n",
      "[ 68.        3.14972   3.21434]                               \n",
      "[ 69.        3.15665   3.21486]                               \n",
      "[ 70.        3.144     3.21766]                               \n",
      "[ 71.        3.14383   3.21135]                               \n",
      "[ 72.        3.13783   3.21155]                               \n",
      "[ 73.        3.16029   3.21576]                               \n",
      "[ 74.        3.13515   3.21186]                               \n",
      "[ 75.        3.13129   3.20888]                               \n",
      "[ 76.        3.12602   3.21095]                               \n",
      "[ 77.        3.13937   3.20334]                               \n",
      "[ 78.        3.14281   3.20578]                               \n",
      "[ 79.        3.11146   3.20254]                               \n",
      "[ 80.        3.12097   3.20556]                               \n",
      "[ 81.        3.13853   3.20288]                               \n",
      "[ 82.        3.11176   3.19771]                               \n",
      "[ 83.        3.10022   3.19899]                               \n",
      "[ 84.        3.08803   3.20024]                               \n",
      "[ 85.        3.10131   3.19341]                               \n",
      "[ 86.        3.09571   3.19726]                               \n",
      "[ 87.        3.08308   3.19288]                               \n",
      "[ 88.        3.08395   3.1949 ]                               \n",
      "[ 89.        3.07491   3.19382]                               \n",
      "[ 90.        3.06486   3.19501]                               \n",
      "[ 91.        3.06683   3.19144]                               \n",
      "[ 92.        3.07857   3.18718]                               \n",
      "[ 93.        3.05401   3.18527]                               \n",
      "[ 94.        3.04677   3.18656]                               \n",
      "[ 95.        3.0553    3.18233]                               \n",
      "[ 96.        3.0488    3.18346]                               \n",
      "[ 97.        3.0287    3.18448]                               \n",
      "[ 98.        3.01797   3.18421]                               \n",
      "[ 99.        3.01875   3.18515]                               \n",
      "[ 100.        3.0135    3.1851]                               \n",
      "[ 101.        3.0187    3.1792]                               \n",
      "[ 102.         3.01974    3.1747 ]                            \n",
      "[ 103.         3.05192    3.17778]                            \n",
      "[ 104.         3.00792    3.17611]                            \n",
      "[ 105.         2.98202    3.18041]                            \n",
      "[ 106.         2.98129    3.17635]                            \n",
      "[ 107.         2.97652    3.1732 ]                            \n",
      "[ 108.         2.96527    3.17854]                            \n",
      "[ 109.         2.964      3.17932]                            \n",
      "[ 110.         2.96853    3.17629]                            \n",
      "[ 111.         2.94722    3.17465]                            \n",
      "[ 112.         3.04352    3.16994]                            \n",
      "[ 113.         2.95431    3.17609]                            \n",
      "[ 114.         2.96986    3.17391]                            \n",
      "[ 115.         2.93348    3.17583]                            \n",
      "[ 116.         2.94597    3.17772]                            \n",
      "[ 117.         2.93355    3.17219]                            \n",
      "[ 118.         2.92284    3.17581]                            \n",
      "[ 119.         2.92699    3.17412]                            \n",
      "[ 120.         2.97235    3.16988]                            \n",
      "[ 121.         2.92528    3.17761]                            \n",
      "[ 122.         2.92446    3.17519]                            \n",
      "[ 123.         2.92002    3.17708]                            \n",
      "[ 124.         2.91397    3.17539]                            \n",
      "[ 125.         2.9398     3.17027]                            \n",
      "[ 126.         2.93207    3.17253]                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learner.fit(3e-3, 7, wds=1e-6, cycle_len=1, cycle_mult=2, cycle_save_name=\"PtH_Stage1_cyclemul2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load_cycle(\"PtH_Stage1_cyclemul2\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.save_encoder('PtH_Stage1_cyclemult_newtest_secondRun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.load_encoder(\"PtH_Stage1_cyclemult_newtest_secondRun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I have two model weights saved as PtH_Stage1_cyclemult_newtest_secondRun and PtH_Stage1_cyclemult_newtest.  This will be the starting point for my final weights which are specifically trained to learn happy and unhappy reviews.  The current weights are just meant to build reviews themselves which is demonstrated below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEXT = pickle.load(open(f'{PATH}models/TEXT3.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This hotel is so'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=learner.model\n",
    "ss=\"\"\"This hotel is so \"\"\"\n",
    "s = [spacy_tok(ss)]\n",
    "t = TEXT.numericalize(s)\n",
    "' '.join(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0].bs = 1\n",
    "m.eval()\n",
    "m.reset()\n",
    "res,*_ = m(t)\n",
    "\n",
    "m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close',\n",
       " 'nice',\n",
       " 'much',\n",
       " 'small',\n",
       " 'convenient',\n",
       " '-',\n",
       " 'good',\n",
       " '<unk>',\n",
       " 'well',\n",
       " 'expensive']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nexts = torch.topk(res[-1], 10)[1]\n",
    "[TEXT.vocab.itos[o] for o in to_np(nexts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This hotel is so  \n",
      "\n",
      "close to the airport , you can get to the airport , and the hotel is very close to the airport . <eos> i stayed at the hotel for - nights in early september . the hotel is in a great location , close to the subway , and the ...\n"
     ]
    }
   ],
   "source": [
    "print(ss,\"\\n\")\n",
    "for i in range(50):\n",
    "    n=res[-1].topk(2)[1]\n",
    "    n = n[1] if n.data[0]==0 else n[0]\n",
    "    print(TEXT.vocab.itos[n.data[0]], end=' ')\n",
    "    res,*_ = m(n[0].unsqueeze(0))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of words makes sense as a review and I could reasonably see this being something on a website. Definitely not perfect, but also reasonable enough that it could be just human error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to create a torchtext dataset since fastai is built to work very closely with torchtext.  This ended up being much easier than I had guessed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictHappinessDataset(torchtext.data.Dataset):\n",
    "    def __init__(self, path, text_field, label_field, **kwargs):\n",
    "        fields = [(\"Description\", text_field), (\"Is_Response\", label_field)]\n",
    "        examples = []\n",
    "        #for label in ['happy', 'not_happy']:\n",
    "        #    for fname in iglob(os.path.join(path, label, '*.txt')):\n",
    "        #        with open(fname, 'r') as f: text = f.readline()\n",
    "        for i in range(trn.values[:,1].shape[0]):\n",
    "            text = trn.Description[i]\n",
    "            label = trn.Is_Response[i]\n",
    "            examples.append(data.Example.fromlist([text, label], fields))#[fields[1], fields[-1]]))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex): return len(ex.Description)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, root='.data',\n",
    "               train='train', test='test', **kwargs):\n",
    "        return super().splits(\n",
    "            root, text_field=text_field, label_field=label_field,\n",
    "            train=train, validation=None, test=test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PH_LABEL = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = PredictHappinessDataset.splits(TEXT, PH_LABEL, PATH, train='trn', test='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where I take the model from above and teach it how to determine happy vs unhappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Description and label name are coming from the dataset I created above these values are being fed in from the CSV\n",
    "#This is an importand difference from reading directly from the CSV. \n",
    "md2 = TextData.from_splits(PATH, splits, bs=bs, text_name=\"Description\", label_name=\"Is_Response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiplier=1\n",
    "dropout=0.1*multiplier\n",
    "dropouti=0.4*multiplier\n",
    "wdrop=0.5*multiplier\n",
    "dropoute=0.05*multiplier\n",
    "dropouth=0.3*multiplier\n",
    "\n",
    "m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl,dropout=dropout, dropouti=dropouti, wdrop=wdrop, dropoute=dropoute, dropouth=dropouth)\n",
    "m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "m3.load_encoder('PtH_Stage1_cyclemult_newtest_secondRun')\n",
    "\n",
    "m3.clip=25.\n",
    "lrs=np.array([1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49d4541769346d69660a3798bc7c5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 31/608 [00:03<01:08,  8.36it/s, loss=0.46] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-57a8dfc68f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#this is from the second run through doubling dictionary traintime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mm3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_save_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"m3cycles\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HackerEarth/PtH/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HackerEarth/PtH/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m---> 89\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HackerEarth/PtH/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HackerEarth/PtH/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HackerEarth/PtH/fastai/lm_rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mraw_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HackerEarth/PtH/fastai/lm_rnn.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(self, arrs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HackerEarth/PtH/fastai/lm_rnn.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(iterable, dim)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mConcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, dim, *inputs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "#this is from the second run through doubling dictionary traintime\n",
    "#m3.freeze_to(-1)\n",
    "#m3.fit(lrs, 4, metrics=[accuracy])\n",
    "#m3.unfreeze()\n",
    "#m3.fit(lrs, 4, metrics=[accuracy], cycle_len=1, cycle_mult=2, cycle_save_name=\"m3cycles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m3.save_encoder(\"m3_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m3.load_encoder(\"m3_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.load(\"m3_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier=1\n",
    "dropout=0.1*multiplier\n",
    "dropouti=0.4*multiplier\n",
    "wdrop=0.5*multiplier\n",
    "dropoute=0.05*multiplier\n",
    "dropouth=0.3*multiplier\n",
    "\n",
    "m4 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl,dropout=dropout, dropouti=dropouti, wdrop=wdrop, dropoute=dropoute, dropouth=dropouth)\n",
    "m4.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "m4.load_encoder('PtH_Stage1_cyclemult_newtest')\n",
    "\n",
    "m4.clip=25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d1a137541a4018824cbf1c09db373b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.40545  0.32259  0.87009]                        \n",
      "[ 1.       0.40197  0.36688  0.85231]                        \n",
      "[ 2.       0.43458  0.27963  0.8843 ]                        \n",
      "[ 3.       0.4265   0.28263  0.88567]                        \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a626cabaa924f739310859c60392e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.31343  0.24236  0.90365]                        \n",
      "[ 1.       0.28469  0.22289  0.91142]                        \n",
      "[ 2.       0.26625  0.2234   0.91293]                        \n",
      "[ 3.       0.26935  0.22339  0.91224]                        \n",
      "[ 4.       0.25794  0.2094   0.91856]                        \n",
      "[ 5.       0.23912  0.20621  0.92018]                        \n",
      "[ 6.       0.23435  0.20324  0.92126]                        \n",
      "[ 7.       0.25121  0.19627  0.92301]                        \n",
      "[ 8.       0.24058  0.19371  0.92254]                        \n",
      "[ 9.       0.23593  0.1848   0.92768]                        \n",
      "[ 10.        0.22597   0.17993   0.92948]                    \n",
      "[ 11.        0.22256   0.17637   0.93118]                    \n",
      "[ 12.        0.2095    0.17469   0.93177]                    \n",
      "[ 13.        0.21815   0.17338   0.9321 ]                    \n",
      "[ 14.        0.20402   0.17356   0.93187]                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#m4.freeze_to(-1)\n",
    "#m4.fit(lrs, 4, metrics=[accuracy])\n",
    "#m4.unfreeze()\n",
    "#m4.fit(lrs, 4, metrics=[accuracy], cycle_len=1, cycle_mult=2, cycle_save_name=\"m4cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m4.save_encoder(\"m4_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4.load(\"m4_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier=1\n",
    "dropout=0.1*multiplier\n",
    "dropouti=0.4*multiplier\n",
    "wdrop=0.5*multiplier\n",
    "dropoute=0.05*multiplier\n",
    "dropouth=0.3*multiplier\n",
    "\n",
    "m5 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl,dropout=dropout, dropouti=dropouti, wdrop=wdrop, dropoute=dropoute, dropouth=dropouth)\n",
    "m5.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "m5.load_encoder('PtH_Stage1_cyclemult_newtest_thirdRun')\n",
    "\n",
    "m5.clip=25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5.load(\"m5_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['User_ID','Is_Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3m = m3.model \n",
    "m4m = m4.model\n",
    "m5m = m5.model\n",
    "m3m[0].bs = 1\n",
    "m4m[0].bs = 1\n",
    "m5m[0].bs = 1\n",
    "for i in range(tst.values[:,1].shape[0]):\n",
    "    ss = tst[\"Description\"][i]\n",
    "    s = [spacy_tok(ss)]\n",
    "    t = TEXT.numericalize(s)\n",
    "   \n",
    "    m3m.eval()\n",
    "    m4m.eval()\n",
    "    m5m.eval()\n",
    "    m3m.reset()\n",
    "    m4m.reset()\n",
    "    m5m.reset()\n",
    "    resm3m,*_ = m3m(t)\n",
    "    resm4m,*_ = m4m(t)\n",
    "    resm5m,*_ = m5m(t)\n",
    "    \n",
    "    res = resm3m+resm4m+resm5m\n",
    "    user_id = tst[\"User_ID\"][i]\n",
    "    is_response = PH_LABEL.vocab.itos[to_np(torch.topk(res[-1], 1)[1])[0]]\n",
    "    submission = submission.append({\"User_ID\":user_id, 'Is_Response':is_response.replace(\" \",\"_\")},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(PATH+\"PredictHappinessModel_Final_ensemble_17_11_29.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
